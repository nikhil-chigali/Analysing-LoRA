# LoRA-BERT

LoRA-BERT is a project that implements Low Rank Adapters (LoRA) for BERT models. LoRA is a technique that reduces the number of parameters in BERT models by introducing low-rank matrices as adapters.

## Table of Contents

- [Introduction](#introduction)
- [Installation](#installation)

## Introduction

LoRA-BERT is designed to provide a more efficient and compact version of BERT models by leveraging low-rank matrices. By using LoRA, you can reduce the memory footprint and computational requirements of BERT models while maintaining their performance.

This project aims to provide an easy-to-use implementation of LoRA for BERT models, allowing researchers and developers to experiment with and benefit from this technique.

## Installation

To install LoRA-BERT, follow these steps:

1. Clone the repository:

    ```shell
    git clone https://github.com/nikhil-chigali/LoRA-BERT.git
    ```

2. Install the required dependencies:

    ```shell
    pip install -r requirements.txt
    ```

